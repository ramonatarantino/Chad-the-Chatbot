{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/TWw70TP/logo.png)\n",
    "\n",
    "# Chad the chatbot\n",
    "\n",
    "## Notebook description\n",
    "\n",
    "1. Creation of a custom dataset by web scraping of the website [Cross Validated - Stack Exchange](https://stats.stackexchange.com).\n",
    "\n",
    "2. Fine-tuning of the [\"EleutherAI/gpt-neox-20b\"](https://huggingface.co/EleutherAI/gpt-neox-20b) model using bitsandbytes, 4-bit quantization and QLoRA.\n",
    "\n",
    "3. Zero-shot-classification with [\"facebook/bart-large-mnli\"](https://huggingface.co/facebook/bart-large-mnli) to filter questions related to our topics.\n",
    "\n",
    "4. Creation of the real chatbot that combines the classification of questions and the possible generation of answers through the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from random import uniform\n",
    "from time import sleep\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all the links of the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page = 4150\n",
    "link_list = []\n",
    "for i in range(1, last_page+1):\n",
    "    link_list.append(\"https://stats.stackexchange.com/questions?tab=votes&page={}\".format(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all the links of the questions\n",
    "\n",
    "Only performed on the first 500 pages, each of which contained 50 questions, all sorted by how \"hot\" the question was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "start = 0\n",
    "end = 500\n",
    "for link in tqdm(link_list[start:end]):\n",
    "    page = requests.get(link)\n",
    "    if page.status_code == 200:\n",
    "        pageParsed = BeautifulSoup(page.content, 'html.parser')\n",
    "        try:\n",
    "            all_page = pageParsed.find_all('div', {'class':'s-post-summary--content'})\n",
    "            for question in all_page:\n",
    "                question_link = question.find('h3', class_='s-post-summary--content-title').find('a')['href']\n",
    "                questions.append('https://stats.stackexchange.com' + question_link)\n",
    "        except:\n",
    "            print('Failed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the links list of the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('questions_links.pkl', 'wb') as file:\n",
    "    pkl.dump(questions, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions: 25000\n"
     ]
    }
   ],
   "source": [
    "with open('questions_links.pkl', 'rb') as file:\n",
    "    questions_links = pkl.load(file)\n",
    "print(f'Total number of questions: {len(questions_links)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dictionary that contains:\n",
    "\n",
    "- index\n",
    "\n",
    "- question\n",
    "\n",
    "- answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(df_dict, idx, start, stop, save=True):\n",
    "    \n",
    "    # Set headers and user agent\n",
    "    headers = {'User-Agent': \n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36'}\n",
    "    page_num = start\n",
    "    for question_link in tqdm(questions_links[start:stop]):\n",
    "        sleep(uniform(0.4, 0.6))\n",
    "\n",
    "        try:\n",
    "            page = requests.get(question_link, headers=headers)\n",
    "            pageParsed = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            question = pageParsed.find('div', {'class': 'd-flex sm:fd-column'}).find('a').text\n",
    "            texts = pageParsed.find_all('div', {'class': 's-prose js-post-body'})\n",
    "\n",
    "            if len(texts) > 1:\n",
    "\n",
    "                for answer in texts[1:]:\n",
    "                    df_dict[idx] = {'question': question, 'answer': answer.text.strip()}\n",
    "                    idx += 1\n",
    "        except:\n",
    "            print(f'Failed page: {page_num}')\n",
    "            \n",
    "        page_num += page_num\n",
    "\n",
    "    if save == True:\n",
    "        with open('df_dict.pkl', 'wb') as file:\n",
    "            pkl.dump(df_dict, file)\n",
    "            \n",
    "    return df_dict, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "idx = 1\n",
    "start = 0\n",
    "stop = 25000\n",
    "df_dict, idx = scrape(df_dict=df_dict, idx=idx, start=start, stop=stop, save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe and then a transformers Dataset from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "      <td>Imagine a big family dinner where everybody st...</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "      <td>The manuscript \"A tutorial on Principal Compon...</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "      <td>Let's do (2) first.  PCA fits an ellipsoid to ...</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "      <td>Hmm, here goes for a completely non-mathematic...</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "      <td>I'd answer in \"layman's terms\" by saying that ...</td>\n",
       "      <td>In today's pattern recognition class my profes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                           question   \n",
       "0    1  In today's pattern recognition class my profes...  \\\n",
       "1    2  In today's pattern recognition class my profes...   \n",
       "2    3  In today's pattern recognition class my profes...   \n",
       "3    4  In today's pattern recognition class my profes...   \n",
       "4    5  In today's pattern recognition class my profes...   \n",
       "\n",
       "                                              answer   \n",
       "0  Imagine a big family dinner where everybody st...  \\\n",
       "1  The manuscript \"A tutorial on Principal Compon...   \n",
       "2  Let's do (2) first.  PCA fits an ellipsoid to ...   \n",
       "3  Hmm, here goes for a completely non-mathematic...   \n",
       "4  I'd answer in \"layman's terms\" by saying that ...   \n",
       "\n",
       "                                        conversation  \n",
       "0  In today's pattern recognition class my profes...  \n",
       "1  In today's pattern recognition class my profes...  \n",
       "2  In today's pattern recognition class my profes...  \n",
       "3  In today's pattern recognition class my profes...  \n",
       "4  In today's pattern recognition class my profes...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = [{'idx': key, 'question': value['question'], 'answer': value['answer']} for key, value in df_dict.items()]\n",
    "df = pd.DataFrame(rows)\n",
    "df['conversation'] = df['question'] + '\\n\\n' + df['answer']\n",
    "print(df.shape)\n",
    "display(df.head())\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pandas dataframe from the dictionary\n",
    "rows = [{'idx': key, 'question': value['question'], 'answer': value['answer']} for key, value in df_dict.items()]\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# reduce the max length of the questions and answers\n",
    "q_max_length = df[\"question\"].str.len().max()\n",
    "print(f'Question max length: {q_max_length}')\n",
    "df['question_cut'] = df['question'].str.slice(0, 100)\n",
    "a_max_length = df[\"answer\"].str.len().max()\n",
    "print(f'Asnwer max length: {a_max_length}')\n",
    "df['answer_cut'] = df['answer'].str.slice(0, 200)\n",
    "\n",
    "# create a new column called 'conversation' joining 'question' and 'answer'\n",
    "df['conversation'] = df['question'] + '\\n' + df['answer']\n",
    "# create a new column called 'conversation_cut' joining 'question_cut' and 'answer_cut'\n",
    "df['conversation_cut'] = df['question_cut'] + '\\n' + df['answer_cut']\n",
    "\n",
    "# resetting the index column\n",
    "df['idx'] = np.arange(1, len(df)+1)\n",
    "df.head()\n",
    "df2 = df[['idx', 'conversation_cut']]\n",
    "\n",
    "# transforming the pandas df in a huggingface DatasetDict\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "dataset2 = Dataset.from_pandas(df2)\n",
    "dataset_dict2 = DatasetDict({'train': dataset2})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dataset\n",
    "\n",
    "*In addition the dataset has also been uploaded to the [Hugging Face](https://huggingface.co) website and can be found here [Prot10/CrossValidated](https://huggingface.co/datasets/Prot10/CrossValidated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pkl', 'wb') as file:\n",
    "    pkl.dump(dataset_dict, file)\n",
    "    \n",
    "with open('dataset_sub.pkl', 'wb') as file:\n",
    "    pkl.dump(dataset_dict2, file)\n",
    "    \n",
    "df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning of the model for text-generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's load the model we are going to use: `GPT-neo-x-20B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Footprint: 10.61 GB\n"
     ]
    }
   ],
   "source": [
    "memory_footprint = model.get_memory_footprint()\n",
    "memory_footprint_gb = memory_footprint / (1024 ** 3)\n",
    "print(f\"Memory Footprint: {memory_footprint_gb:.2f} GB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=20, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset previously created (from Hugging Face)\n",
    "\n",
    "*N.B. skip this cell if you have already loaded the dataset from the previous part...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Prot10/CrossValidated\")\n",
    "data = dataset.map(lambda samples: tokenizer(samples[\"conversation\"]), batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*and just run...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_dict2.map(lambda samples: tokenizer(samples[\"conversation\"]), batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 08:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.933200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.771300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.553700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.8552961707115174, metrics={'train_runtime': 547.1652, 'train_samples_per_second': 0.439, 'train_steps_per_second': 0.037, 'total_flos': 994427072593920.0, 'train_loss': 2.8552961707115174, 'epoch': 0.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=6,\n",
    "        warmup_steps=4,\n",
    "        max_steps=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the fine-tuned model\n",
    "\n",
    "*In addition the dataset has also been uploaded to the [Hugging Face](https://huggingface.co) website and can be found here [Prot10/chad](https://huggingface.co/Prot10/chad)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "model_to_save.save_pretrained(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now we can try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "What is the definition of expected value?\n",
      "\n",
      "\n",
      "I am trying to understand the definition of expected value.\n",
      "I have seen the definition of expected value as:\n",
      "\n",
      "The expected value of a random variable X is the sum of the probabilities of all possible outcomes of X, multiplied by the value of the outcome.\n",
      "I am trying\n"
     ]
    }
   ],
   "source": [
    "text = 'What is the definition of expected value?'\n",
    "device = \"cuda:0\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=60)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for zero-shot-classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model we are going to use: `bart-large-mnli`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some parameters and give it a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      "{'math': 0.6207872033119202, 'probs': 0.2595915198326111, 'other': 0.05335698649287224, 'stats': 0.027889791876077652, 'machine-learning': 0.021786486729979515, 'greetings': 0.01658804342150688} \n",
      "\n",
      "It's an appropriate question!\n"
     ]
    }
   ],
   "source": [
    "candidate_labels = ['greetings', 'stats', 'probs', 'math', 'machine-learning', 'other']\n",
    "threshold = 0.5\n",
    "\n",
    "sequence_to_classify = \"what is an integral?\"\n",
    "\n",
    "result = classifier(sequence_to_classify, candidate_labels)\n",
    "labels = dict(zip(result['labels'], result['scores']))\n",
    "print('Predicted labels:')\n",
    "print(labels, '\\n')\n",
    "if labels['greetings'] + labels['other'] < threshold:\n",
    "    print(\"It's an appropriate question!\")\n",
    "else:\n",
    "    print(\"It's not an appropriate question!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to run the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, threshold=0.5, max_new_tokens=60):\n",
    "    \n",
    "    candidate_labels = ['greetings', 'stats', 'probs', 'math', 'machine-learning', 'other']\n",
    "    \n",
    "    result = classifier(question, candidate_labels)\n",
    "    labels = dict(zip(result['labels'], result['scores']))\n",
    "\n",
    "    if labels['greetings'] + labels['other'] < threshold:\n",
    "        device = \"cuda:0\"\n",
    "        inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        print(f'\\n\\n\\033[1mCHAD:\\033[0m {tokenizer.decode(outputs[0], skip_special_tokens=True)[len(question)+2:]}')\n",
    "    \n",
    "    elif labels['greetings'] > 0.5:\n",
    "        print(\"\\033[1mCHAD:\\033[0m Hi, I'm Chad and I was created to help you deepen concepts related to the world of Data Science, ask me what you need!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\033[1mCHAD:\\033[0m I'm sorry but the question asked does not fall within the topics to which I can answer. If you think this is a mistake please try to rephrase the question differently!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And finally run the Chatbot with a few different examples\n",
    "\n",
    "- **Example 1:**\n",
    "\n",
    "  Question: \"What is the definition of expected value?\"\n",
    "\n",
    "  Class: \"probs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCHAD:\u001b[0m I am trying to understand the definition of expected value.\n",
      "I have seen the definition of expected value as:\n",
      "\n",
      "The expected value of a random variable X is the sum of the probabilities of all possible outcomes of X, multiplied by the value of the outcome.\n",
      "I am trying\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the definition of expected value?\"\n",
    "chat(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example 2:**\n",
    "\n",
    "  Question: \"Hi, can I ask you a question?\"\n",
    "\n",
    "  Class: \"greetings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCHAD:\u001b[0m Hi, I'm Chad and I was created to help you deepen concepts related to the world of Data Science, ask me what you need!\n"
     ]
    }
   ],
   "source": [
    "question = \"Hi, what's your name?\"\n",
    "chat(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example 3:**\n",
    "\n",
    "  Question: \"What is a Blue Glaucus?\"\n",
    "\n",
    "  Class: \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCHAD:\u001b[0m I'm sorry but the question asked does not fall within the topics to which I can answer. If you think this is a mistake please try to rephrase the question differently!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a Blue Glaucus?\"\n",
    "chat(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try now the model on Colab\n",
    "\n",
    "You can find the notebook here $\\rightsquigarrow$ [CHAD](https://colab.research.google.com/drive/1k8wHhCZkePFECJp1AOaPZ4ZH8BiZGtwq?usp=share_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
